{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn import preprocessing\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "userProfile_train = pd.read_csv('../data/userProfile_train.csv')\n",
    "action_train = pd.read_csv('../data/action_train.csv')\n",
    "orderHistory_train = pd.read_csv('../data/orderHistory_train.csv')\n",
    "orderFuture_train = pd.read_csv('../data/orderFuture_train.csv')\n",
    "userComment_train = pd.read_csv('../data/userComment_train.csv')\n",
    "\n",
    "userProfile_test = pd.read_csv('../data/userProfile_test.csv')\n",
    "action_test = pd.read_csv('../data/action_test.csv')\n",
    "orderHistory_test = pd.read_csv('../data/orderHistory_test.csv')\n",
    "orderFuture_test = pd.read_csv('../data/orderFuture_test.csv')\n",
    "userComment_test = pd.read_csv('../data/userComment_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print' (<ipython-input-8-17d0eeeb3f65>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-8-17d0eeeb3f65>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    print orderFuture_train.shape\u001b[0m\n\u001b[1;37m                          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m Missing parentheses in call to 'print'\n"
     ]
    }
   ],
   "source": [
    "print orderFuture_train.shape \n",
    "print orderFuture_test.shape\n",
    "le = preprocessing.LabelBinarizer()\n",
    "encoder1 = le.fit(pd.concat([orderHistory_train,orderHistory_test]).country.values)\n",
    "\n",
    "le = preprocessing.LabelBinarizer()\n",
    "encoder2 = le.fit(pd.concat([orderHistory_train,orderHistory_test]).continent.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getHistoryFeature(data, encoder1, encoder2):\n",
    "    df = copy.deepcopy(data)\n",
    "    #--get order feature\n",
    "    feature = df.groupby('userid')['orderType'].agg(['sum','count']).reset_index().rename(columns={'sum': 'order_num_1', 'count': 'order_num'})\n",
    "    feature['order_num_0'] = feature['order_num'] - feature['order_num_1']\n",
    "    feature['order_ratio_0'] = feature['order_num_0'].astype('float')/feature['order_num']\n",
    "    feature['order_ratio_1'] = feature['order_num_1'].astype('float')/feature['order_num']\n",
    "    f = copy.deepcopy(feature)\n",
    "    \n",
    "    #--get total feature\n",
    "    feature = df.groupby('userid')['city','country','continent'].count().reset_index().rename(\n",
    "        columns={'city': 'city_num', 'country': 'country_num','continent':'continent_num'})\n",
    "    f = pd.merge(f,feature, on = 'userid', how = 'left')\n",
    "    feature = df[df.orderType == 1].groupby('userid')['city','country','continent'].count().reset_index().rename(\n",
    "        columns={'city': 'city_num_1', 'country': 'country_num_1','continent':'continent_num_1'})\n",
    "    f = pd.merge(f,feature, on = 'userid', how = 'left').fillna(0)\n",
    "    for val in ['city_num', 'country_num', 'continent_num']:\n",
    "        f[val.split('_')[0]+'_ratio_1'] = f[val+'_1'].astype('float')/f[val]\n",
    "    #--get country feature\n",
    "#     le = preprocessing.LabelBinarizer()\n",
    "    country_encoder = encoder1.transform(df.country.values)\n",
    "    country_encoder_col = ['country_%d'%i for i in range(country_encoder.shape[1])]\n",
    "    df1 = pd.DataFrame(country_encoder, columns = country_encoder_col)\n",
    "    df1['userid'] = df['userid'].values\n",
    "    feature = df1.groupby('userid')[country_encoder_col].agg(['sum','count']).reset_index()\n",
    "    f = pd.merge(f,feature, on = 'userid', how = 'left')\n",
    "    \n",
    "    #--get continent feature\n",
    "#     le = preprocessing.LabelBinarizer()\n",
    "    continent_encoder = encoder2.transform(df.continent.values)\n",
    "    continent_encoder_col = ['continent_%d'%i for i in range(continent_encoder.shape[1])]\n",
    "    df1 = pd.DataFrame(continent_encoder, columns = continent_encoder_col)\n",
    "    df1['userid'] = df['userid'].values\n",
    "    feature = df1.groupby('userid')[continent_encoder_col].agg(['sum','count']).reset_index()\n",
    "    f = pd.merge(f,feature, on = 'userid', how = 'left')\n",
    "    \n",
    "    \n",
    "    #--get orderTime last feature\n",
    "#     df1 = df.groupby('userid').apply(lambda x: x.sort_values('orderTime', ascending = False).head(1)).reset_index(drop = True)[['userid','orderid','orderTime','orderType']]\n",
    "#     df1.columns = [['userid','last_orderid','last_orderTime','last_orderType']]\n",
    "#     f = pd.merge(f, df1, on = 'userid',how = 'left')\n",
    "    \n",
    "    #--get orderTime last 5 feature\n",
    "#     df1 = df.groupby('userid').apply(lambda x: x.sort_values('orderTime', ascending = False).head(5)).reset_index(drop = True)[['userid','orderid','orderTime','orderType']]\n",
    "#     df1.columns = [['userid','last_orderid','last_orderTime','last_orderType']]\n",
    "#     temp = pd.concat([df1,df1.groupby('userid').rank(method = 'first').astype('int').reset_index().rename(\n",
    "#             columns={'last_orderTime': 'last_orderTime_rank'})['last_orderTime_rank']],axis = 1)\n",
    "    \n",
    "#     ff1 = temp.pivot('userid','last_orderTime_rank','last_orderType')\n",
    "#     ff1.columns = ['his_type%d'%i for i in range(ff1.shape[1])]\n",
    "#     ff1 = ff1.reset_index()\n",
    "#     f = pd.merge(f, ff1, on = 'userid',how = 'left')\n",
    "    \n",
    "#     ff2 = temp.pivot('userid','last_orderTime_rank','last_orderTime')\n",
    "#     ff2.columns = ['his_time%d'%i for i in range(ff2.shape[1])]\n",
    "#     ff2 = ff2.reset_index()\n",
    "#     f = pd.merge(f, ff2, on = 'userid',how = 'left')\n",
    "    \n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getUserProfileFeature(df):\n",
    "    le = preprocessing.LabelBinarizer()\n",
    "    encoder = le.fit_transform(df.gender.fillna('_NA_').values)\n",
    "    encoder_col = ['gender_%d'%i for i in range(encoder.shape[1])]\n",
    "    df1 = pd.DataFrame(encoder, columns = encoder_col)\n",
    "    df1['userid'] = df['userid'].values\n",
    "    f = df1\n",
    "    \n",
    "    le = preprocessing.LabelBinarizer()\n",
    "    encoder = le.fit_transform(df.province.fillna('_NA_').values)\n",
    "    encoder_col = ['province_%d'%i for i in range(encoder.shape[1])]\n",
    "    df1 = pd.DataFrame(encoder, columns = encoder_col)\n",
    "    df1['userid'] = df['userid'].values\n",
    "    f = pd.merge(f,df1, on = 'userid', how = 'left')\n",
    "    \n",
    "    le = preprocessing.LabelBinarizer()\n",
    "    encoder = le.fit_transform(df.age.fillna('_NA_').values)\n",
    "    encoder_col = ['age_%d'%i for i in range(encoder.shape[1])]\n",
    "    df1 = pd.DataFrame(encoder, columns = encoder_col)\n",
    "    df1['userid'] = df['userid'].values\n",
    "    f = pd.merge(f,df1, on = 'userid', how = 'left')\n",
    "#     print f.head()\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getActionFeature(data):\n",
    "    df = copy.deepcopy(data)\n",
    "    #---all action feature\n",
    "    result = df.groupby(['userid','actionType'])['actionTime'].count().reset_index().rename(\n",
    "        columns = {'actionTime':'actionNum'}).pivot('userid','actionType','actionNum').apply(lambda x: x/np.sum(x))\n",
    "    result.columns = ['action_type_num%d'%i for i in range(result.shape[1])]\n",
    "    result = result.reset_index()\n",
    "    \n",
    "    #---get feature by unix time\n",
    "    for window in [6]:\n",
    "        print window\n",
    "        print 'actiontype'\n",
    "        f = df.groupby('userid').apply(lambda x: x.sort_values('actionTime', ascending = False).head(window)).reset_index(drop = True).rename(\n",
    "            columns={'actionType': 'actionType_last', 'actionTime': 'actionTime_last'})\n",
    "        f2 = pd.concat([f,f.groupby('userid').rank(method = 'first').astype('int').reset_index().rename(\n",
    "                columns={'actionTime_last': 'actionTime_last_rank'})['actionTime_last_rank']],axis = 1)\n",
    "        \n",
    "        ff1 = f2.pivot('userid','actionTime_last_rank','actionType_last')\n",
    "        ff1_diff = ff1.diff(1, axis = 1)\n",
    "        ff1_diff.columns = ['last%d_type_1diff%d'%(window,i) for i in range(ff1_diff.shape[1])]\n",
    "        ff1_diff = ff1_diff.iloc[:,1:].reset_index()\n",
    "        result = pd.merge(result, ff1_diff, on = 'userid', how = 'left')\n",
    "        \n",
    "        ff1['last%d_type_max'%window] = ff1.max(axis = 0)\n",
    "        ff1['last%d_type_min'%window] = ff1.min(axis = 0)\n",
    "        ff1['last%d_type_meam'%window] = ff1.mean(axis = 0)\n",
    "        ff1['last%d_type_median'%window] = ff1.median(axis = 0)\n",
    "        ff1['last%d_type_std'%window] = ff1.std(axis = 0)\n",
    "        ff1['last%d_type_sum'%window] = ff1.sum(axis = 0)\n",
    "        \n",
    "        ff1.columns = ['last%d_type%d'%(window,i) for i in range(ff1.shape[1])]\n",
    "        ff1 = ff1.reset_index()\n",
    "        \n",
    "        result = pd.merge(result, ff1, on = 'userid', how = 'left')\n",
    "        \n",
    "\n",
    "        print 'actiontime'\n",
    "        ff2 = f2.pivot('userid','actionTime_last_rank','actionTime_last')\n",
    "        ff2_diff = ff2.diff(1, axis = 1)\n",
    "        ff2_diff.columns = ['last%d_time_1diff%d'%(window,i) for i in range(ff2_diff.shape[1])]\n",
    "        ff2_diff = ff2_diff.iloc[:,1:].reset_index()\n",
    "        result = pd.merge(result, ff2_diff, on = 'userid', how = 'left')\n",
    "        \n",
    "        \n",
    "\n",
    "        ff2['last%d_time_max'%window] = ff2.max(axis = 0)\n",
    "        ff2['last%d_time_min'%window] = ff2.min(axis = 0)\n",
    "        ff2['last%d_time_meam'%window] = ff2.mean(axis = 0)\n",
    "        ff2['last%d_time_median'%window] = ff2.median(axis = 0)\n",
    "        ff2['last%d_time_std'%window] = ff2.std(axis = 0)\n",
    "        ff2['last%d_time_sum'%window] = ff2.sum(axis = 0)\n",
    "        \n",
    "        ff2.columns = ['last%d_time%d'%(window,i) for i in range(ff2.shape[1])]\n",
    "        ff2 = ff2.reset_index()\n",
    "        result = pd.merge(result, ff2, on = 'userid', how = 'left')\n",
    "       \n",
    "        \n",
    "        ff = f.groupby(['userid','actionType_last'])['actionTime_last'].count().reset_index().rename(\n",
    "            columns = {'actionTime_last':'actionNum_last'}).pivot('userid','actionType_last','actionNum_last').apply(lambda x: x/np.sum(x))\n",
    "        ff.columns = ['last%d_action_type_num%d'%(window,i) for i in range(ff.shape[1])]\n",
    "        ff = ff.reset_index()\n",
    "        result = pd.merge(result, ff, on = 'userid', how = 'left')\n",
    "        \n",
    "    #---sort every type last 1 action feature\n",
    "    f = df.groupby(['userid','actionType']).apply(lambda x: x.sort_values('actionTime', ascending = False).head(1)).reset_index(drop = True).rename(\n",
    "        columns={'actionTime': 'type_actionTime_last'})\n",
    "    ff3 = f.pivot('userid','actionType','type_actionTime_last')\n",
    "    \n",
    "    ff3_diff = ff3.diff(1, axis = 1)\n",
    "    ff3_diff.columns = ['type_%d_lsttime_diff'%i for i in range(ff3_diff.shape[1])]\n",
    "    ff3_diff = ff3_diff.iloc[:,1:].reset_index()\n",
    "    ff3.columns = ['type_%d_lasttime'%i for i in range(ff3.shape[1])]\n",
    "    ff3 = ff3.reset_index()\n",
    "    result = pd.merge(result, ff3, on = 'userid', how = 'left')\n",
    "    \n",
    "    for t in [1,2,3,4,5,6,7,8,9]:\n",
    "        print t\n",
    "        window = 5\n",
    "        df_type = df[df.actionType == t]\n",
    "        f = df_type.groupby('userid').apply(lambda x: x.sort_values('actionTime', ascending = False).head(window)).reset_index(drop = True).rename(\n",
    "            columns={'actionTime': 'actionTime_last'})\n",
    "        f2 = pd.concat([f,f.groupby('userid').rank(method = 'first').astype('int').reset_index().rename(\n",
    "                columns={'actionTime_last': 'actionTime_last_rank'})['actionTime_last_rank']],axis = 1)\n",
    "        \n",
    "        ff1 = f2.pivot('userid','actionTime_last_rank','actionTime_last')\n",
    "        ff1_diff = ff1.diff(1, axis = 1)\n",
    "        ff1_diff.columns = ['last%d_type%d_time_diff%d'%(window,t,i) for i in range(ff1_diff.shape[1])]\n",
    "        ff1_diff = ff1_diff.iloc[:,1:].reset_index()\n",
    "        \n",
    "        ff1['last%d_type%d_max'%(window,t)] = ff1.max(axis = 0)\n",
    "        ff1['last%d_type%d_min'%(window,t)] = ff1.min(axis = 0)\n",
    "        ff1['last%d_type%d_meam'%(window,t)] = ff1.mean(axis = 0)\n",
    "        ff1['last%d_type%d_median'%(window,t)] = ff1.median(axis = 0)\n",
    "        ff1['last%d_type%d_std'%(window,t)] = ff1.std(axis = 0)\n",
    "        ff1['last%d_type%d_sum'%(window,t)] = ff1.sum(axis = 0)\n",
    "        \n",
    "        ff1.columns = ['last%d_type%d_time%d'%(window,t,i) for i in range(ff1.shape[1])]\n",
    "        ff1 = ff1.reset_index()\n",
    "        \n",
    "        result = pd.merge(result, ff1, on = 'userid', how = 'left')\n",
    "        result = pd.merge(result, ff1_diff, on = 'userid', how = 'left')\n",
    "    #--get feature by date\n",
    "#     print 'date feature'\n",
    "#     df['date'] = pd.to_datetime(df['actionTime'],unit='s').dt.date\n",
    "#     for window in [40]:\n",
    "#         print window\n",
    "#         df_select = df[df.date >= pd.bdate_range(end=df.date.max(), periods=window).date[0]]\n",
    "#         print df_select.shape\n",
    "#         f = df_select.groupby('userid').apply(lambda x: x.sort_values('actionTime', ascending = False).head(6)).reset_index(drop = True).rename(\n",
    "#                 columns={'actionType': 'actionType_last', 'actionTime': 'actionTime_last'})\n",
    "#         f2 = pd.concat([f,f.groupby('userid').rank(method = 'first').astype('int').reset_index().rename(\n",
    "#                 columns={'actionTime_last': 'actionTime_last_rank'})['actionTime_last_rank']],axis = 1)\n",
    "        \n",
    "#         ff1 = f2.pivot('userid','actionTime_last_rank','actionType_last')\n",
    "#         ff1_diff = ff1.diff(1, axis = 1)\n",
    "#         ff1_diff.columns = ['lastdate%d_type_diff%d'%(window,i) for i in range(ff1_diff.shape[1])]\n",
    "#         ff1_diff = ff1_diff.iloc[:,1:].reset_index()\n",
    "        \n",
    "#         ff1['lastdate%d_type_max'%window] = ff1.max(axis = 0)\n",
    "#         ff1['lastdate%d_type_min'%window] = ff1.min(axis = 0)\n",
    "#         ff1['lastdate%d_type_meam'%window] = ff1.mean(axis = 0)\n",
    "#         ff1.columns = ['lastdate%d_type%d'%(window,i) for i in range(ff1.shape[1])]\n",
    "#         ff1 = ff1.reset_index()\n",
    "        \n",
    "#         result = pd.merge(result, ff1, on = 'userid', how = 'left')\n",
    "#         result = pd.merge(result, ff1_diff, on = 'userid', how = 'left')\n",
    "        \n",
    "#         ff2 = f2.pivot('userid','actionTime_last_rank','actionTime_last')\n",
    "#         ff2_diff = ff2.diff(1, axis = 1)\n",
    "#         ff2_diff.columns = ['last%d_time_diff%d'%(window,i) for i in range(ff2_diff.shape[1])]\n",
    "#         ff2_diff = ff2_diff.iloc[:,1:].reset_index()\n",
    "\n",
    "#         ff2['last%d_time_max'%window] = ff2.max(axis = 0)\n",
    "#         ff2['last%d_time_min'%window] = ff2.min(axis = 0)\n",
    "#         ff2['last%d_time_meam'%window] = ff2.mean(axis = 0)\n",
    "#         ff2.columns = ['last%d_time%d'%(window,i) for i in range(ff2.shape[1])]\n",
    "#         ff2 = ff2.reset_index()\n",
    "#         result = pd.merge(result, ff2, on = 'userid', how = 'left')\n",
    "#         result = pd.merge(result, ff2_diff, on = 'userid', how = 'left')\n",
    "        \n",
    "        \n",
    "#         ff = f.groupby(['userid','actionType_last'])['actionTime_last'].count().reset_index().rename(\n",
    "#             columns = {'actionTime_last':'actionNum_last'}).pivot('userid','actionType_last','actionNum_last').apply(lambda x: x/np.sum(x))\n",
    "#         ff.columns = ['last%d_action_type_num%d'%(window,i) for i in range(ff.shape[1])]\n",
    "#         ff = ff.reset_index()\n",
    "#         result = pd.merge(result, ff, on = 'userid', how = 'left')\n",
    "\n",
    "    \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "def getCommentFeature(data):\n",
    "    df = copy.deepcopy(data)\n",
    "    feature = df.groupby('userid')['rating'].agg(['max','min','mean','sum','count','median','std']).reset_index().rename(\n",
    "        columns={'max': 'rate_max', 'min': 'rate_min','mean':'rate_mean','sum':'rate_sum','count':'rate_count','median':'rate_median','std':'rate_std'})\n",
    "    \n",
    "#     mlb = MultiLabelBinarizer()\n",
    "#     tagsV = []\n",
    "#     for line in df.tags.values:\n",
    "#         if line == line:\n",
    "# #             print line\n",
    "#             tagsV.append(set(line.split('|')))\n",
    "#         else:\n",
    "#             tagsV.append(set(''))\n",
    "            \n",
    "    \n",
    "#     tagsF = mlb.fit_transform(tagsV)\n",
    "#     name = []\n",
    "#     for i in range(tagsF.shape[1]):\n",
    "#         df['tag_%d'%i] = tagsF[:,i]\n",
    "#         name.append('tag_%d'%i)\n",
    "    \n",
    "#     f = df.groupby('userid')[name].agg(['mean','sum','count','std']).reset_index()\n",
    "    \n",
    "#     feature = pd.merge(feature, f, on= 'userid', how = 'left') \n",
    "    \n",
    "    \n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = getHistoryFeature(pd.concat([orderHistory_train, orderHistory_test]), encoder1, encoder2)\n",
    "train = pd.merge(orderFuture_train, history, on = 'userid', how = 'left')\n",
    "test = pd.merge(orderFuture_test, history, on = 'userid', how = 'left')\n",
    "print train.shape, test.shape\n",
    "\n",
    "profile = getUserProfileFeature(pd.concat([userProfile_train, userProfile_test]))\n",
    "train = pd.merge(train, profile, on = 'userid', how = 'left')\n",
    "test = pd.merge(test, profile, on = 'userid', how = 'left')\n",
    "print train.shape, test.shape\n",
    "\n",
    "comment = getCommentFeature(pd.concat([userComment_train, userComment_test]))\n",
    "train = pd.merge(train, comment, on = 'userid', how = 'left')\n",
    "test = pd.merge(test, comment, on = 'userid', how = 'left')\n",
    "print train.shape, test.shape\n",
    "\n",
    "action = getActionFeature(pd.concat([action_train, action_test]))\n",
    "train = pd.merge(train, action, on = 'userid', how = 'left')\n",
    "test = pd.merge(test, action, on = 'userid', how = 'left')\n",
    "print train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_x = train.drop(['orderType'], axis = 1)\n",
    "train_y = train.orderType.values\n",
    "print train_x.shape, len(train_y)\n",
    "\n",
    "param = {}\n",
    "param['booster'] = 'gbtree'\n",
    "param['objective'] = 'binary:logistic'\n",
    "param['eval_metric'] = 'auc'\n",
    "param['stratified'] = 'True'\n",
    "param['eta'] = 0.02\n",
    "param['silent'] = 1\n",
    "param['max_depth'] = 5\n",
    "param['subsample'] = 0.7\n",
    "param['colsample_bytree'] = 0.8\n",
    "# param['lambda'] = 2\n",
    "# param['min_child_weight'] = 10\n",
    "param['scale_pos_weight'] = 1\n",
    "param['seed'] = 1024\n",
    "param['nthread'] = 16\n",
    "\n",
    "dtrain = xgb.DMatrix(train_x, label=train_y)\n",
    "# res = xgb.cv(param, dtrain, 3500, nfold = 5, early_stopping_rounds=100, verbose_eval = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print res.shape[0]\n",
    "model = xgb.train(param, dtrain, res.shape[0], evals=[(dtrain,'train')], verbose_eval=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = test[train_x.columns.values]\n",
    "dtest = xgb.DMatrix(test_x)\n",
    "y = model.predict(dtest)\n",
    "test['orderType']=y\n",
    "test[['userid','orderType']].to_csv('../result/xgb_baseline_local9594.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_f = model.get_fscore()\n",
    "imp_df = pd.DataFrame({'feature':[key for key, value in imp_f.items()], 'fscore':[value for key, value in imp_f.items()]})\n",
    "imp_df = imp_df.sort_values(by = 'fscore',ascending=False)\n",
    "imp_df.to_csv('../imp/xgb_imp.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_x = train.drop(['orderType'], axis = 1)\n",
    "train_y = train.orderType.values\n",
    "print train_x.shape, len(train_y)\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "param = {}\n",
    "param['task'] = 'train'\n",
    "param['boosting_type'] = 'gbdt'\n",
    "param['objective'] = 'binary'\n",
    "param['metric'] = 'auc'\n",
    "param['min_sum_hessian_in_leaf'] = 0.1\n",
    "param['learning_rate'] = 0.01\n",
    "param['verbosity'] = 2\n",
    "param['tree_learner'] = 'feature'\n",
    "param['num_leaves'] = 128\n",
    "param['feature_fraction'] = 0.7\n",
    "param['bagging_fraction'] = 0.7\n",
    "param['bagging_freq'] = 1\n",
    "param['num_threads'] = 16\n",
    "\n",
    "\n",
    "dtrain = lgb.Dataset(train_x, label=train_y)\n",
    "res1gb = lgb.cv(param, dtrain, 5500, nfold = 5, early_stopping_rounds=100, verbose_eval = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ro = len(res1gb['auc-mean'])\n",
    "model = lgb.train(param, dtrain, ro, valid_sets=[dtrain], verbose_eval=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = test[train_x.columns.values]\n",
    "y = model.predict(test_x)\n",
    "test['orderType']=y\n",
    "test[['userid','orderType']].to_csv('../result/lgb_baseline_local9619.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
